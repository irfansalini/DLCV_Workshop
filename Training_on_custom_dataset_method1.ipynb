{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irfansalini/DLCV_Workshop/blob/main/Training_on_custom_dataset_method1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "gu5EoMTZpGPe"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/dataset1/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbNqjEc1etMr",
        "outputId": "bf791d11-c72d-4578-a4d9-f6237004d5cb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /dataset1/; to attempt to forcibly remount, call drive.mount(\"/dataset1/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rokxI_qOpGPk"
      },
      "source": [
        "## Structure of our data folder\n",
        "\n",
        "For this exercise, we’ll keep the following folder structure:\n",
        "\n",
        "<div> <img src=\"https://github.com/CUTe-EmbeddedAI/DLCV_Workshop/blob/main/220720_day6/images/fig47.png?raw=1\" alt=\"Drawing\" style=\"width: 300px;\"/></div> \n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig47.png)\n",
        "\n",
        "This is a straightforward folder structure with a root folder as the Train/Test folders containing classes with images inside them. \n",
        "\n",
        "*However, some other dataset, as you’ll see in the future, might have a slightly different structure. It doesn’t matter in what structure we get the data in. The data can all be in a single folder with class names in the image names (like “Cat_001.jpg”) or even in a CSV, we can process all this in our custom dataset class.\n",
        "\n",
        "Let's apply some transformations to our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Veui89ripGPp"
      },
      "outputs": [],
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        #transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.Resize(size=(128,128)),\n",
        "        #transforms.RandomRotation(degrees=15),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        #transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=(128,128)),\n",
        "        #transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoUb9dAOpGPq"
      },
      "source": [
        "## Method 1: Define dataset using `torchvision.datasets.ImageFolder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWbs8c-pGPr",
        "outputId": "2301bc1b-9f79-473b-c67c-d8ae80731a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "# dataset = '/content/drive/My Drive/01. TEACHING/MACHINE_VISION/code/fruit_dataset'\n",
        "dataset = '/dataset1/MyDrive/dataset1/'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'valid')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 8\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "# idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "# print(idx_to_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAEEe58pGPs"
      },
      "source": [
        "Let's see the info on train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4m0JrIXpGPt",
        "outputId": "b22ad958-2818-4a49-cc33-738bd54aa54d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 3066\n",
              "    Root location: /dataset1/MyDrive/dataset1/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "data['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "JuZoH8SVpGPu"
      },
      "outputs": [],
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "# valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "# valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "lllGMRytpGPv",
        "outputId": "365fb533-479f-4e92-9548-b3c70fa64105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3066, 766)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "train_data_size, test_data_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "7pHq9YTbpGPw"
      },
      "outputs": [],
      "source": [
        "input_size = (3,32,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "jTuCiNMepGPx",
        "outputId": "c260868e-dde1-4a96-b6a0-44243ae838ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3066\n",
            "766\n"
          ]
        }
      ],
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=data['train'], # use custom created train Dataset\n",
        "                                     batch_size=4, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=data['test'], # use custom created test Dataset\n",
        "                                    batch_size=4, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "z7TYXzQ4pGPy",
        "outputId": "bcedc97e-f647-4a27-e13c-8bca9bd1062a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (batchnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=4608, out_features=84, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "#######################\n",
        "# DEFINE YOUR OWN MODEL\n",
        "#######################\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(6)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 84)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 32 * 12 * 12)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        #x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.3)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "bEVE8du9pGPz"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "k9tt7xW0pGP0",
        "outputId": "a7ccfe86-f444-42ae-b670-0ad846da4752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 000, Training: Loss: 1.9385, Accuracy: 47.8474%, \n",
            "\t\tValidation : Loss : 1.8224, Accuracy: 51.4360%, Time: 63.2065s\n",
            "Epoch: 2/30\n",
            "Epoch : 001, Training: Loss: 1.5232, Accuracy: 50.7175%, \n",
            "\t\tValidation : Loss : 1.4618, Accuracy: 57.5718%, Time: 62.2017s\n",
            "Epoch: 3/30\n",
            "Epoch : 002, Training: Loss: 1.2143, Accuracy: 61.0894%, \n",
            "\t\tValidation : Loss : 1.2144, Accuracy: 69.5822%, Time: 61.9755s\n",
            "Epoch: 4/30\n",
            "Epoch : 003, Training: Loss: 1.0085, Accuracy: 72.7658%, \n",
            "\t\tValidation : Loss : 1.0370, Accuracy: 78.4595%, Time: 62.2126s\n",
            "Epoch: 5/30\n",
            "Epoch : 004, Training: Loss: 0.8750, Accuracy: 75.8969%, \n",
            "\t\tValidation : Loss : 0.8846, Accuracy: 80.5483%, Time: 62.1518s\n",
            "Epoch: 6/30\n",
            "Epoch : 005, Training: Loss: 0.7796, Accuracy: 77.5277%, \n",
            "\t\tValidation : Loss : 0.7756, Accuracy: 84.3342%, Time: 62.2792s\n",
            "Epoch: 7/30\n",
            "Epoch : 006, Training: Loss: 0.7101, Accuracy: 79.7782%, \n",
            "\t\tValidation : Loss : 0.7065, Accuracy: 85.6397%, Time: 62.6647s\n",
            "Epoch: 8/30\n",
            "Epoch : 007, Training: Loss: 0.6535, Accuracy: 81.1807%, \n",
            "\t\tValidation : Loss : 0.6385, Accuracy: 89.0339%, Time: 62.4943s\n",
            "Epoch: 9/30\n",
            "Epoch : 008, Training: Loss: 0.6009, Accuracy: 83.4964%, \n",
            "\t\tValidation : Loss : 0.5928, Accuracy: 90.9922%, Time: 62.1432s\n",
            "Epoch: 10/30\n",
            "Epoch : 009, Training: Loss: 0.5513, Accuracy: 84.8663%, \n",
            "\t\tValidation : Loss : 0.5324, Accuracy: 91.3838%, Time: 62.5165s\n",
            "Epoch: 11/30\n",
            "Epoch : 010, Training: Loss: 0.5101, Accuracy: 85.6164%, \n",
            "\t\tValidation : Loss : 0.4586, Accuracy: 93.0809%, Time: 62.2269s\n",
            "Epoch: 12/30\n",
            "Epoch : 011, Training: Loss: 0.4739, Accuracy: 86.6275%, \n",
            "\t\tValidation : Loss : 0.4263, Accuracy: 92.8198%, Time: 62.4516s\n",
            "Epoch: 13/30\n",
            "Epoch : 012, Training: Loss: 0.4410, Accuracy: 87.1494%, \n",
            "\t\tValidation : Loss : 0.4113, Accuracy: 93.6031%, Time: 61.7613s\n",
            "Epoch: 14/30\n",
            "Epoch : 013, Training: Loss: 0.4164, Accuracy: 87.4755%, \n",
            "\t\tValidation : Loss : 0.3544, Accuracy: 93.6031%, Time: 62.3807s\n",
            "Epoch: 15/30\n",
            "Epoch : 014, Training: Loss: 0.3921, Accuracy: 87.7365%, \n",
            "\t\tValidation : Loss : 0.3200, Accuracy: 94.3864%, Time: 62.3455s\n",
            "Epoch: 16/30\n",
            "Epoch : 015, Training: Loss: 0.3751, Accuracy: 87.7691%, \n",
            "\t\tValidation : Loss : 0.3012, Accuracy: 94.7781%, Time: 62.5713s\n",
            "Epoch: 17/30\n",
            "Epoch : 016, Training: Loss: 0.3515, Accuracy: 88.8454%, \n",
            "\t\tValidation : Loss : 0.2777, Accuracy: 94.6475%, Time: 63.1747s\n",
            "Epoch: 18/30\n",
            "Epoch : 017, Training: Loss: 0.3428, Accuracy: 88.6171%, \n",
            "\t\tValidation : Loss : 0.2637, Accuracy: 94.2559%, Time: 63.2472s\n",
            "Epoch: 19/30\n",
            "Epoch : 018, Training: Loss: 0.3290, Accuracy: 88.8780%, \n",
            "\t\tValidation : Loss : 0.2398, Accuracy: 94.3864%, Time: 62.8088s\n",
            "Epoch: 20/30\n",
            "Epoch : 019, Training: Loss: 0.3190, Accuracy: 89.4977%, \n",
            "\t\tValidation : Loss : 0.2364, Accuracy: 95.1697%, Time: 63.1563s\n",
            "Epoch: 21/30\n",
            "Epoch : 020, Training: Loss: 0.3142, Accuracy: 88.9759%, \n",
            "\t\tValidation : Loss : 0.2133, Accuracy: 95.5614%, Time: 63.0215s\n",
            "Epoch: 22/30\n",
            "Epoch : 021, Training: Loss: 0.3083, Accuracy: 89.4651%, \n",
            "\t\tValidation : Loss : 0.2023, Accuracy: 95.4308%, Time: 63.1136s\n",
            "Epoch: 23/30\n",
            "Epoch : 022, Training: Loss: 0.2997, Accuracy: 89.6282%, \n",
            "\t\tValidation : Loss : 0.1932, Accuracy: 95.3003%, Time: 63.3774s\n",
            "Epoch: 24/30\n",
            "Epoch : 023, Training: Loss: 0.2949, Accuracy: 89.4325%, \n",
            "\t\tValidation : Loss : 0.2162, Accuracy: 95.3003%, Time: 62.9913s\n",
            "Epoch: 25/30\n",
            "Epoch : 024, Training: Loss: 0.2825, Accuracy: 90.2805%, \n",
            "\t\tValidation : Loss : 0.1850, Accuracy: 95.3003%, Time: 63.2943s\n",
            "Epoch: 26/30\n",
            "Epoch : 025, Training: Loss: 0.2797, Accuracy: 90.5414%, \n",
            "\t\tValidation : Loss : 0.1878, Accuracy: 95.3003%, Time: 62.9767s\n",
            "Epoch: 27/30\n",
            "Epoch : 026, Training: Loss: 0.2792, Accuracy: 90.1826%, \n",
            "\t\tValidation : Loss : 0.1795, Accuracy: 95.6919%, Time: 63.2374s\n",
            "Epoch: 28/30\n",
            "Epoch : 027, Training: Loss: 0.2691, Accuracy: 90.7045%, \n",
            "\t\tValidation : Loss : 0.1760, Accuracy: 95.8225%, Time: 63.4367s\n",
            "Epoch: 29/30\n",
            "Epoch : 028, Training: Loss: 0.2655, Accuracy: 90.9654%, \n",
            "\t\tValidation : Loss : 0.1698, Accuracy: 96.0836%, Time: 62.9741s\n",
            "Epoch: 30/30\n",
            "Epoch : 029, Training: Loss: 0.2549, Accuracy: 91.1937%, \n",
            "\t\tValidation : Loss : 0.1670, Accuracy: 96.0836%, Time: 63.2416s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 30\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "KeEEFPgzcp_e",
        "outputId": "b9622196-2071-434d-e631-b052d04489b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "CS4DbmvScp_e",
        "outputId": "9a8b3b55-ff76-4dcf-a67a-d6a00f3a4b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBTMMjIpGP1"
      },
      "source": [
        "## Method 2: Define custom dataset in Pytorch\n",
        "\n",
        "Pytorch has a great ecosystem to load custom datasets for training machine learning models.\n",
        "\n",
        "This is a straightforward folder structure with a root folder as the Train/Test folders containing classes with images inside them. As we’ll see, it doesn’t matter in what structure we get the data in. The data can all be in a single folder with class names in the image names (like “Cat_001.jpg”) or even in a CSV, we can process all this in our custom dataset class.\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig47.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTAtSNnIcp_l"
      },
      "source": [
        "## Inference on webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "CkqPput5cp_m"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "P9oQ3tpfcp_m"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Y5E8-Mwicp_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3753e93b-6159-490a-ece2-a9672245d743"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no mask 0.8945601582527161\n",
            "no mask 0.8930049538612366\n",
            "no mask 0.8920559883117676\n",
            "no mask 0.8943389058113098\n",
            "no mask 0.8913196921348572\n",
            "no mask 0.8900517821311951\n",
            "no mask 0.8869924545288086\n",
            "no mask 0.8937026858329773\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "categories = ['no mask','mask']\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    transform = image_transforms['train']\n",
        "    frame = Image.fromarray(frame)\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 2 categories that are predicted.\n",
        "    top2_prob, top2_catid = torch.topk(probabilities, 2)\n",
        "    \n",
        "    print(categories[top2_catid[0]], top2_prob[0].item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5VyWBAIFz_-J"
      },
      "execution_count": 148,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Training_on_custom_dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7 (pytorch_hasan)",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}